# ğŸ“¦ Order Tracking Event Driven Data Ingestion 

This project implements a robust, file-triggered data pipeline in **Databricks** using **Delta Lake**. It efficiently stages and loads order tracking data from raw CSV files into Delta tables with upsert logic for incremental processing.

---

## ğŸ“Š Architecture Overview

The pipeline uses two sequential notebooks in a **Databricks Workflow**:

```
order_stage_load â” orders_target_load
```

Each notebook is responsible for a part of the end-to-end processing:

* `order_stage_load`: Reads incoming CSV files and stages them
* `orders_target_load`: Performs a Delta Lake merge into the target table

---

## ğŸ“ Input & Output

| Component         | Path / Table Location                                    |
| ----------------- | -------------------------------------------------------- |
| ğŸ”¹ Source Folder  | `/Volumes/incremental_load/default/orders_data/source/`  |
| ğŸ“¤ Archive Folder | `/Volumes/incremental_load/default/orders_data/archive/` |
| ğŸ” Stage Table    | `incremental_load.default.orders_stage`                  |
| ğŸ“ˆ Target Table   | `incremental_load.default.orders_target`                 |

---

## ğŸ“’ Notebook 1: `order_stage_load`

### ğŸ“™ Description:

* Reads CSV files from the source folder
* Applies schema to ensure correctness
* Writes data to the staging Delta table
* Moves processed files to the archive directory

### ğŸ”§ Features:

* Enforced schema using `StructType`
* Timestamp parsing support
* File archival using `dbutils.fs.mv`
* Overwrites the staging table for freshness
* Files are **automatically moved** to the archive directory after being processed

---

## ğŸ“’ Notebook 2: `orders_target_load`

### ğŸ“™ Description:

* Reads data from the stage table
* Performs upsert using `MERGE` on `tracking_num`
* Creates target table if it doesnâ€™t exist

### ğŸ”§ Merge Logic:

* Deletes matched rows (`whenMatchedDelete()`)
* Appends remaining data to the target table

---

## â° Scheduling

Triggered **every minute** using file arrival on:

```
/Volumes/incremental_load/default/orders_data/source/
```

---

## âš™ï¸ Technologies Used

* Apache Spark (PySpark)
* Delta Lake
* Databricks Workflows
* GitHub Integration

---

## ğŸš€ Example Use Case

Tracking shipments and updates from vendors:

* Source CSVs represent shipment updates
* Processed in near real-time
* Maintains an up-to-date master table

---

## ğŸ”„ Future Enhancements

* Schema evolution support
* Append-only mode with CDC
* Add email/SMS alerts on job failure
* Add row-level audit columns for history tracking

---
